{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. umair, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is blue.', 'You have to eat pizza.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "TEXT = \"Hello Mr. umair, how are you doing today? The weather is great, and Python is awesome. The sky is blue. You have to eat pizza.\"\n",
    "print(sent_tokenize(TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr.',\n",
       " 'umair',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?',\n",
       " 'The',\n",
       " 'weather',\n",
       " 'is',\n",
       " 'great',\n",
       " ',',\n",
       " 'and',\n",
       " 'Python',\n",
       " 'is',\n",
       " 'awesome',\n",
       " '.',\n",
       " 'The',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'blue',\n",
       " '.',\n",
       " 'You',\n",
       " 'have',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'pizza',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=word_tokenize(TEXT)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: Hello Mr. umair , how are you doing...>\n"
     ]
    }
   ],
   "source": [
    "from nltk.text import Text\n",
    "t = Text(tokens)\n",
    "print (t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.count('is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'is': 3, '.': 3, ',': 2, 'The': 2, 'Hello': 1, 'Mr.': 1, 'umair': 1, 'how': 1, 'are': 1, 'you': 1, ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "# filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "#or\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'إذ\\nإذا\\nإذما\\nإذن\\nأف\\nأقل\\nأكثر\\nألا\\nإلا\\nالتي\\nالذي\\nالذين\\nاللاتي\\nاللائي\\nاللتان\\nاللتيا\\nاللتين\\nاللذان\\nاللذين\\nاللواتي\\nإلى\\nإليك\\nإليكم\\nإليكما\\nإليكن\\nأم\\nأما\\nأما\\nإما\\nأن\\nإن\\nإنا\\nأنا\\nأنت\\nأنتم\\nأنتما\\nأنتن\\nإنما\\nإنه\\nأنى\\nأنى\\nآه\\nآها\\nأو\\nأولاء\\nأولئك\\nأوه\\nآي\\nأي\\nأيها\\nإي\\nأين\\nأين\\nأينما\\nإيه\\nبخ\\nبس\\nبعد\\nبعض\\nبك\\nبكم\\nبكم\\nبكما\\nبكن\\nبل\\nبلى\\nبما\\nبماذا\\nبمن\\nبنا\\nبه\\nبها\\nبهم\\nبهما\\nبهن\\nبي\\nبين\\nبيد\\nتلك\\nتلكم\\nتلكما\\nته\\nتي\\nتين\\nتينك\\nثم\\nثمة\\nحاشا\\nحبذا\\nحتى\\nحيث\\nحيثما\\nحين\\nخلا\\nدون\\nذا\\nذات\\nذاك\\nذان\\nذانك\\nذلك\\nذلكم\\nذلكما\\nذلكن\\nذه\\nذو\\nذوا\\nذواتا\\nذواتي\\nذي\\nذين\\nذينك\\nريث\\nسوف\\nسوى\\nشتان\\nعدا\\nعسى\\nعل\\nعلى\\nعليك\\nعليه\\nعما\\nعن\\nعند\\nغير\\nفإذا\\nفإن\\nفلا\\nفمن\\nفي\\nفيم\\nفيما\\nفيه\\nفيها\\nقد\\nكأن\\nكأنما\\nكأي\\nكأين\\nكذا\\nكذلك\\nكل\\nكلا\\nكلاهما\\nكلتا\\nكلما\\nكليكما\\nكليهما\\nكم\\nكم\\nكما\\nكي\\nكيت\\nكيف\\nكيفما\\nلا\\nلاسيما\\nلدى\\nلست\\nلستم\\nلستما\\nلستن\\nلسن\\nلسنا\\nلعل\\nلك\\nلكم\\nلكما\\nلكن\\nلكنما\\nلكي\\nلكيلا\\nلم\\nلما\\nلن\\nلنا\\nله\\nلها\\nلهم\\nلهما\\nلهن\\nلو\\nلولا\\nلوما\\nلي\\nلئن\\nليت\\nليس\\nليسا\\nليست\\nليستا\\nليسوا\\nما\\nماذا\\nمتى\\nمذ\\nمع\\nمما\\nممن\\nمن\\nمنه\\nمنها\\nمنذ\\nمه\\nمهما\\nنحن\\nنحو\\nنعم\\nها\\nهاتان\\nهاته\\nهاتي\\nهاتين\\nهاك\\nهاهنا\\nهذا\\nهذان\\nهذه\\nهذي\\nهذين\\nهكذا\\nهل\\nهلا\\nهم\\nهما\\nهن\\nهنا\\nهناك\\nهنالك\\nهو\\nهؤلاء\\nهي\\nهيا\\nهيت\\nهيهات\\nوالذي\\nوالذين\\nوإذ\\nوإذا\\nوإن\\nولا\\nولكن\\nولو\\nوما\\nومن\\nوهو\\nيا\\nأبٌ\\nأخٌ\\nحمٌ\\nفو\\nأنتِ\\nيناير\\nفبراير\\nمارس\\nأبريل\\nمايو\\nيونيو\\nيوليو\\nأغسطس\\nسبتمبر\\nأكتوبر\\nنوفمبر\\nديسمبر\\nجانفي\\nفيفري\\nمارس\\nأفريل\\nماي\\nجوان\\nجويلية\\nأوت\\nكانون\\nشباط\\nآذار\\nنيسان\\nأيار\\nحزيران\\nتموز\\nآب\\nأيلول\\nتشرين\\nدولار\\nدينار\\nريال\\nدرهم\\nليرة\\nجنيه\\nقرش\\nمليم\\nفلس\\nهللة\\nسنتيم\\nيورو\\nين\\nيوان\\nشيكل\\nواحد\\nاثنان\\nثلاثة\\nأربعة\\nخمسة\\nستة\\nسبعة\\nثمانية\\nتسعة\\nعشرة\\nأحد\\nاثنا\\nاثني\\nإحدى\\nثلاث\\nأربع\\nخمس\\nست\\nسبع\\nثماني\\nتسع\\nعشر\\nثمان\\nسبت\\nأحد\\nاثنين\\nثلاثاء\\nأربعاء\\nخميس\\nجمعة\\nأول\\nثان\\nثاني\\nثالث\\nرابع\\nخامس\\nسادس\\nسابع\\nثامن\\nتاسع\\nعاشر\\nحادي\\nأ\\nب\\nت\\nث\\nج\\nح\\nخ\\nد\\nذ\\nر\\nز\\nس\\nش\\nص\\nض\\nط\\nظ\\nع\\nغ\\nف\\nق\\nك\\nل\\nم\\nن\\nه\\nو\\nي\\nء\\nى\\nآ\\nؤ\\nئ\\nأ\\nة\\nألف\\nباء\\nتاء\\nثاء\\nجيم\\nحاء\\nخاء\\nدال\\nذال\\nراء\\nزاي\\nسين\\nشين\\nصاد\\nضاد\\nطاء\\nظاء\\nعين\\nغين\\nفاء\\nقاف\\nكاف\\nلام\\nميم\\nنون\\nهاء\\nواو\\nياء\\nهمزة\\nي\\nنا\\nك\\nكن\\nه\\nإياه\\nإياها\\nإياهما\\nإياهم\\nإياهن\\nإياك\\nإياكما\\nإياكم\\nإياك\\nإياكن\\nإياي\\nإيانا\\nأولالك\\nتانِ\\nتانِك\\nتِه\\nتِي\\nتَيْنِ\\nثمّ\\nثمّة\\nذانِ\\nذِه\\nذِي\\nذَيْنِ\\nهَؤلاء\\nهَاتانِ\\nهَاتِه\\nهَاتِي\\nهَاتَيْنِ\\nهَذا\\nهَذانِ\\nهَذِه\\nهَذِي\\nهَذَيْنِ\\nالألى\\nالألاء\\nأل\\nأنّى\\nأيّ\\nّأيّان\\nأنّى\\nأيّ\\nّأيّان\\nذيت\\nكأيّ\\nكأيّن\\nبضع\\nفلان\\nوا\\nآمينَ\\nآهِ\\nآهٍ\\nآهاً\\nأُفٍّ\\nأُفٍّ\\nأفٍّ\\nأمامك\\nأمامكَ\\nأوّهْ\\nإلَيْكَ\\nإلَيْكَ\\nإليكَ\\nإليكنّ\\nإيهٍ\\nبخٍ\\nبسّ\\nبَسْ\\nبطآن\\nبَلْهَ\\nحاي\\nحَذارِ\\nحيَّ\\nحيَّ\\nدونك\\nرويدك\\nسرعان\\nشتانَ\\nشَتَّانَ\\nصهْ\\nصهٍ\\nطاق\\nطَق\\nعَدَسْ\\nكِخ\\nمكانَك\\nمكانَك\\nمكانَك\\nمكانكم\\nمكانكما\\nمكانكنّ\\nنَخْ\\nهاكَ\\nهَجْ\\nهلم\\nهيّا\\nهَيْهات\\nوا\\nواهاً\\nوراءَك\\nوُشْكَانَ\\nوَيْ\\nيفعلان\\nتفعلان\\nيفعلون\\nتفعلون\\nتفعلين\\nاتخذ\\nألفى\\nتخذ\\nترك\\nتعلَّم\\nجعل\\nحجا\\nحبيب\\nخال\\nحسب\\nخال\\nدرى\\nرأى\\nزعم\\nصبر\\nظنَّ\\nعدَّ\\nعلم\\nغادر\\nذهب\\nوجد\\nورد\\nوهب\\nأسكن\\nأطعم\\nأعطى\\nرزق\\nزود\\nسقى\\nكسا\\nأخبر\\nأرى\\nأعلم\\nأنبأ\\nحدَث\\nخبَّر\\nنبَّا\\nأفعل به\\nما أفعله\\nبئس\\nساء\\nطالما\\nقلما\\nلات\\nلكنَّ\\nءَ\\nأجل\\nإذاً\\nأمّا\\nإمّا\\nإنَّ\\nأنًّ\\nأى\\nإى\\nأيا\\nب\\nثمَّ\\nجلل\\nجير\\nرُبَّ\\nس\\nعلًّ\\nف\\nكأنّ\\nكلَّا\\nكى\\nل\\nلات\\nلعلَّ\\nلكنَّ\\nلكنَّ\\nم\\nنَّ\\nهلّا\\nوا\\nأل\\nإلّا\\nت\\nك\\nلمّا\\nن\\nه\\nو\\nا\\nي\\nتجاه\\nتلقاء\\nجميع\\nحسب\\nسبحان\\nشبه\\nلعمر\\nمثل\\nمعاذ\\nأبو\\nأخو\\nحمو\\nفو\\nمئة\\nمئتان\\nثلاثمئة\\nأربعمئة\\nخمسمئة\\nستمئة\\nسبعمئة\\nثمنمئة\\nتسعمئة\\nمائة\\nثلاثمائة\\nأربعمائة\\nخمسمائة\\nستمائة\\nسبعمائة\\nثمانمئة\\nتسعمائة\\nعشرون\\nثلاثون\\nاربعون\\nخمسون\\nستون\\nسبعون\\nثمانون\\nتسعون\\nعشرين\\nثلاثين\\nاربعين\\nخمسين\\nستين\\nسبعين\\nثمانين\\nتسعين\\nبضع\\nنيف\\nأجمع\\nجميع\\nعامة\\nعين\\nنفس\\nلا سيما\\nأصلا\\nأهلا\\nأيضا\\nبؤسا\\nبعدا\\nبغتة\\nتعسا\\nحقا\\nحمدا\\nخلافا\\nخاصة\\nدواليك\\nسحقا\\nسرا\\nسمعا\\nصبرا\\nصدقا\\nصراحة\\nطرا\\nعجبا\\nعيانا\\nغالبا\\nفرادى\\nفضلا\\nقاطبة\\nكثيرا\\nلبيك\\nمعاذ\\nأبدا\\nإزاء\\nأصلا\\nالآن\\nأمد\\nأمس\\nآنفا\\nآناء\\nأنّى\\nأول\\nأيّان\\nتارة\\nثمّ\\nثمّة\\nحقا\\nصباح\\nمساء\\nضحوة\\nعوض\\nغدا\\nغداة\\nقطّ\\nكلّما\\nلدن\\nلمّا\\nمرّة\\nقبل\\nخلف\\nأمام\\nفوق\\nتحت\\nيمين\\nشمال\\nارتدّ\\nاستحال\\nأصبح\\nأضحى\\nآض\\nأمسى\\nانقلب\\nبات\\nتبدّل\\nتحوّل\\nحار\\nرجع\\nراح\\nصار\\nظلّ\\nعاد\\nغدا\\nكان\\nما انفك\\nما برح\\nمادام\\nمازال\\nمافتئ\\nابتدأ\\nأخذ\\nاخلولق\\nأقبل\\nانبرى\\nأنشأ\\nأوشك\\nجعل\\nحرى\\nشرع\\nطفق\\nعلق\\nقام\\nكرب\\nكاد\\nهبّ'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.raw('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'إذ إذا إذما إذن أف أقل أكثر ألا إلا التي الذي الذين اللاتي اللائي اللتان اللتيا اللتين اللذان اللذين اللواتي إلى إليك إليكم إليكما إليكن أم أما أما إما أن إن إنا أنا أنت أنتم أنتما أنتن إنما إنه أنى أنى آه آها أو أولاء أولئك أوه آي أي أيها إي أين أين أينما إيه بخ بس بعد بعض بك بكم بكم بكما بكن بل بلى بما بماذا بمن بنا به بها بهم بهما بهن بي بين بيد تلك تلكم تلكما ته تي تين تينك ثم ثمة حاشا حبذا حتى حيث حيثما حين خلا دون ذا ذات ذاك ذان ذانك ذلك ذلكم ذلكما ذلكن ذه ذو ذوا ذواتا ذواتي ذي ذين ذينك ريث سوف سوى شتان عدا عسى عل على عليك عليه عما عن عند غير فإذا فإن فلا فمن في فيم فيما فيه فيها قد كأن كأنما كأي كأين كذا كذلك كل كلا كلاهما كلتا كلما كليكما كليهما كم كم كما كي كيت كيف كيفما لا لاسيما لدى لست لستم لستما لستن لسن لسنا لعل لك لكم لكما لكن لكنما لكي لكيلا لم لما لن لنا له لها لهم لهما لهن لو لولا لوما لي لئن ليت ليس ليسا ليست ليستا ليسوا ما ماذا متى مذ مع مما ممن من منه منها منذ مه مهما نحن نحو نعم ها هاتان هاته هاتي هاتين هاك هاهنا هذا هذان هذه هذي هذين هكذا هل هلا هم هما هن هنا هناك هنالك هو هؤلاء هي هيا هيت هيهات والذي والذين وإذ وإذا وإن ولا ولكن ولو وما ومن وهو يا أبٌ أخٌ حمٌ فو أنتِ يناير فبراير مارس أبريل مايو يونيو يوليو أغسطس سبتمبر أكتوبر نوفمبر ديسمبر جانفي فيفري مارس أفريل ماي جوان جويلية أوت كانون شباط آذار نيسان أيار حزيران تموز آب أيلول تشرين دولار دينار ريال درهم ليرة جنيه قرش مليم فلس هللة سنتيم يورو ين يوان شيكل واحد اثنان ثلاثة أربعة خمسة ستة سبعة ثمانية تسعة عشرة أحد اثنا اثني إحدى ثلاث أربع خمس ست سبع ثماني تسع عشر ثمان سبت أحد اثنين ثلاثاء أربعاء خميس جمعة أول ثان ثاني ثالث رابع خامس سادس سابع ثامن تاسع عاشر حادي أ ب ت ث ج ح خ د ذ ر ز س ش ص ض ط ظ ع غ ف ق ك ل م ن ه و ي ء ى آ ؤ ئ أ ة ألف باء تاء ثاء جيم حاء خاء دال ذال راء زاي سين شين صاد ضاد طاء ظاء عين غين فاء قاف كاف لام ميم نون هاء واو ياء همزة ي نا ك كن ه إياه إياها إياهما إياهم إياهن إياك إياكما إياكم إياك إياكن إياي إيانا أولالك تانِ تانِك تِه تِي تَيْنِ ثمّ ثمّة ذانِ ذِه ذِي ذَيْنِ هَؤلاء هَاتانِ هَاتِه هَاتِي هَاتَيْنِ هَذا هَذانِ هَذِه هَذِي هَذَيْنِ الألى الألاء أل أنّى أيّ ّأيّان أنّى أيّ ّأيّان ذيت كأيّ كأيّن بضع فلان وا آمينَ آهِ آهٍ آهاً أُفٍّ أُفٍّ أفٍّ أمامك أمامكَ أوّهْ إلَيْكَ إلَيْكَ إليكَ إليكنّ إيهٍ بخٍ بسّ بَسْ بطآن بَلْهَ حاي حَذارِ حيَّ حيَّ دونك رويدك سرعان شتانَ شَتَّانَ صهْ صهٍ طاق طَق عَدَسْ كِخ مكانَك مكانَك مكانَك مكانكم مكانكما مكانكنّ نَخْ هاكَ هَجْ هلم هيّا هَيْهات وا واهاً وراءَك وُشْكَانَ وَيْ يفعلان تفعلان يفعلون تفعلون تفعلين اتخذ ألفى تخذ ترك تعلَّم جعل حجا حبيب خال حسب خال درى رأى زعم صبر ظنَّ عدَّ علم غادر ذهب وجد ورد وهب أسكن أطعم أعطى رزق زود سقى كسا أخبر أرى أعلم أنبأ حدَث خبَّر نبَّا أفعل به ما أفعله بئس ساء طالما قلما لات لكنَّ ءَ أجل إذاً أمّا إمّا إنَّ أنًّ أى إى أيا ب ثمَّ جلل جير رُبَّ س علًّ ف كأنّ كلَّا كى ل لات لعلَّ لكنَّ لكنَّ م نَّ هلّا وا أل إلّا ت ك لمّا ن ه و ا ي تجاه تلقاء جميع حسب سبحان شبه لعمر مثل معاذ أبو أخو حمو فو مئة مئتان ثلاثمئة أربعمئة خمسمئة ستمئة سبعمئة ثمنمئة تسعمئة مائة ثلاثمائة أربعمائة خمسمائة ستمائة سبعمائة ثمانمئة تسعمائة عشرون ثلاثون اربعون خمسون ستون سبعون ثمانون تسعون عشرين ثلاثين اربعين خمسين ستين سبعين ثمانين تسعين بضع نيف أجمع جميع عامة عين نفس لا سيما أصلا أهلا أيضا بؤسا بعدا بغتة تعسا حقا حمدا خلافا خاصة دواليك سحقا سرا سمعا صبرا صدقا صراحة طرا عجبا عيانا غالبا فرادى فضلا قاطبة كثيرا لبيك معاذ أبدا إزاء أصلا الآن أمد أمس آنفا آناء أنّى أول أيّان تارة ثمّ ثمّة حقا صباح مساء ضحوة عوض غدا غداة قطّ كلّما لدن لمّا مرّة قبل خلف أمام فوق تحت يمين شمال ارتدّ استحال أصبح أضحى آض أمسى انقلب بات تبدّل تحوّل حار رجع راح صار ظلّ عاد غدا كان ما انفك ما برح مادام مازال مافتئ ابتدأ أخذ اخلولق أقبل انبرى أنشأ أوشك جعل حرى شرع طفق علق قام كرب كاد هبّ'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.raw('arabic').replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n"
     ]
    }
   ],
   "source": [
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\"]\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"It is important to by very pythoner womens while you are pythoning with python. All pythoners have pythoned poorly at least once. there are foxes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Semester 2\\Programming For AI(python)\\Practice\\NLP(NLTK)\\NLTK_CLass.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m words \u001b[39m=\u001b[39m word_tokenize(new_text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m words:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(ps\u001b[39m.\u001b[39mstem(w))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Shahz/nltk_data'\n    - 'c:\\\\Users\\\\Shahz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\Shahz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Shahz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Shahz\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Shahz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Shahz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Shahz/nltk_data'\n    - 'c:\\\\Users\\\\Shahz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\Shahz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Shahz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Shahz\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Shahz\\OneDrive\\Desktop\\Semester 2\\Programming For AI(python)\\Practice\\NLP(NLTK)\\NLTK_CLass.ipynb Cell 17\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shahz/OneDrive/Desktop/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m words \u001b[39m=\u001b[39m word_tokenize(new_text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shahz/OneDrive/Desktop/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m words:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Shahz/OneDrive/Desktop/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(wnl\u001b[39m.\u001b[39;49mlemmatize(w))\n",
      "File \u001b[1;32mc:\\Users\\Shahz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatize\u001b[39m(\u001b[39mself\u001b[39m, word: \u001b[39mstr\u001b[39m, pos: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m     \u001b[39m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[39m=\u001b[39m wn\u001b[39m.\u001b[39;49m_morphy(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmin\u001b[39m(lemmas, key\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m) \u001b[39mif\u001b[39;00m lemmas \u001b[39melse\u001b[39;00m word\n",
      "File \u001b[1;32mc:\\Users\\Shahz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\Shahz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Shahz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Shahz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Shahz/nltk_data'\n    - 'c:\\\\Users\\\\Shahz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\Shahz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Shahz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Shahz\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(wnl.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Semester 2\\Programming For AI(python)\\Practice\\NLP(NLTK)\\NLTK_CLass.ipynb Cell 18\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlancaster\u001b[39;00m \u001b[39mimport\u001b[39;00m LancasterStemmer\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mporter\u001b[39;00m \u001b[39mimport\u001b[39;00m PorterStemmer\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(nltk\u001b[39m.\u001b[39mword_tokenize(\u001b[39m\"\u001b[39m\u001b[39mThe women running in the fog passed bunnies working as computer scientists.\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m snowball \u001b[39m=\u001b[39m SnowballStemmer(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m lancaster \u001b[39m=\u001b[39m LancasterStemmer()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "#Different stemmers\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "text = list(nltk.word_tokenize(\"The women running in the fog passed bunnies working as computer scientists.\"))\n",
    "\n",
    "snowball = SnowballStemmer('english')\n",
    "lancaster = LancasterStemmer()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for stemmer in (snowball, lancaster, porter):\n",
    "    stemmed_text = [stemmer.stem(t) for t in text]\n",
    "    print (\" \".join(stemmed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Shahz/nltk_data'\n    - 'c:\\\\Users\\\\Shahz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\Shahz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Shahz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Shahz\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Shahz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Shahz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Shahz/nltk_data'\n    - 'c:\\\\Users\\\\Shahz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\Shahz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Shahz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Shahz\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Shahz\\OneDrive\\Desktop\\Semester 2\\Programming For AI(python)\\Practice\\NLP(NLTK)\\NLTK_CLass.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Shahz/OneDrive/Desktop/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords \u001b[39mand\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m punctuation:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Shahz/OneDrive/Desktop/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             \u001b[39myield\u001b[39;00m token\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Shahz/OneDrive/Desktop/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39mlist\u001b[39;49m(normalize(\u001b[39m\"\u001b[39;49m\u001b[39mThe EAGLE flies at midnight.\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\n",
      "\u001b[1;32mc:\\Users\\Shahz\\OneDrive\\Desktop\\Semester 2\\Programming For AI(python)\\Practice\\NLP(NLTK)\\NLTK_CLass.ipynb Cell 19\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shahz/OneDrive/Desktop/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nltk\u001b[39m.\u001b[39mword_tokenize(text):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shahz/OneDrive/Desktop/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     token \u001b[39m=\u001b[39m token\u001b[39m.\u001b[39mlower()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Shahz/OneDrive/Desktop/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     token \u001b[39m=\u001b[39m lemmatizer\u001b[39m.\u001b[39;49mlemmatize(token)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Shahz/OneDrive/Desktop/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords \u001b[39mand\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m punctuation:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Shahz/OneDrive/Desktop/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39myield\u001b[39;00m token\n",
      "File \u001b[1;32mc:\\Users\\Shahz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatize\u001b[39m(\u001b[39mself\u001b[39m, word: \u001b[39mstr\u001b[39m, pos: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m     \u001b[39m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[39m=\u001b[39m wn\u001b[39m.\u001b[39;49m_morphy(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmin\u001b[39m(lemmas, key\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m) \u001b[39mif\u001b[39;00m lemmas \u001b[39melse\u001b[39;00m word\n",
      "File \u001b[1;32mc:\\Users\\Shahz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\Shahz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Shahz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Shahz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Shahz/nltk_data'\n    - 'c:\\\\Users\\\\Shahz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\Shahz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Shahz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Shahz\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#Typical normalization of text for use as features in machine learning models looks something like this:\n",
    "import string\n",
    "lemmatizer  = WordNetLemmatizer()\n",
    "stopwords   = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = string.punctuation\n",
    "def normalize(text):\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        token = token.lower()\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "        if token not in stopwords and token not in punctuation:\n",
    "            yield token\n",
    "print (list(normalize(\"The EAGLE flies at midnight.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\Semester 2\\Programming For AI(python)\\Practice\\NLP(NLTK)\\NLTK_CLass.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtextblob\u001b[39;00m \u001b[39mimport\u001b[39;00m TextBlob\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThe women runnning in the foug passed bunnies working as computer scientists.\u001b[39m\u001b[39m\"\u001b[39m               \u001b[39m# Reading the file\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Semester%202/Programming%20For%20AI%28python%29/Practice/NLP%28NLTK%29/NLTK_CLass.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m textBlb \u001b[39m=\u001b[39m TextBlob(text)            \u001b[39m# Making our first textblob\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "       \n",
    "text = \"The women runnning in the foug passed bunnies working as computer scientists.\"               # Reading the file\n",
    "textBlb = TextBlob(text)            # Making our first textblob\n",
    "textCorrected = textBlb.correct()   # Correcting the text\n",
    "print(textCorrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punctuation = string.punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hello',) 1\n",
      "('Mr.',) 1\n",
      "('umair',) 1\n",
      "(',',) 2\n",
      "('how',) 1\n",
      "('are',) 1\n",
      "('you',) 1\n",
      "('doing',) 1\n",
      "('today',) 1\n",
      "('?',) 1\n",
      "('The',) 2\n",
      "('weather',) 1\n",
      "('is',) 3\n",
      "('great',) 1\n",
      "('and',) 1\n",
      "('Python',) 1\n",
      "('awesome',) 1\n",
      "('.',) 3\n",
      "('sky',) 1\n",
      "('blue',) 1\n",
      "('You',) 1\n",
      "('have',) 1\n",
      "('to',) 1\n",
      "('eat',) 1\n",
      "('pizza',) 1\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams, FreqDist\n",
    "unigrams = ngrams(tokens,1)\n",
    "unigramFD  =FreqDist(unigrams)\n",
    "\n",
    "for token in list(unigramFD.items()):\n",
    "    print(token[0], token[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "-5\n"
     ]
    }
   ],
   "source": [
    "x  = 37\n",
    "y = 9\n",
    "print(x//y)\n",
    "print(-x//y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
